# RAG-FAQ-Bot# RAG FAQ Tutor

RAG FAQ Tutor is a small Retrieval-Augmented Generation (RAG) project that turns any tutorial or article URL into an interactive FAQ tutor. Paste a link, load it once, and then ask focused questions grounded only in that page's content. [web:148][web:154]

---

## What this project does

- Lets you **paste any public URL** (e.g., a Java tutorial or docs page).
- Fetches and **indexes the content** of that page in a RAG backend.
- Provides a **chat-style UI** where you can ask questions.
- Answers are generated by an LLM but **constrained to the loaded page**, reducing hallucinations. [attached_file:1][web:148]

A typical example:

- You load `https://www.devacetech.com/insights/java-code-examples`. [web:134]
- The backend ingests that page and stores chunks like “Loops and Conditional Logic”.
- You ask: “give code for loops”.
- The app returns the `for` and `while` loop code snippets **directly from that article**, not from general model memory. [web:134]

---

## Architecture overview

The app is split into:

### 1. Frontend (React)

- **Tech**: React + TypeScript, Vite/CRA, plain CSS.
- **Responsibilities**:
  - Render the single-page UI with:
    - Header: “RAG FAQ Tutor” + subtitle.
    - **Source URL** input + **Load** button.
    - Status banner for ingest success / error.
    - Chat window with user/bot bubbles.
    - Question textarea with **Ctrl+Enter** to send. [web:115][web:116]
  - Manage local state:
    - `url`: current URL to ingest.
    - `question`: current user question text.
    - `messages`: array of `{ role: "user" | "bot"; content: string }`.
    - `loadingUrl`, `loadingAnswer`: booleans to disable buttons and show spinners.
  - Call the backend:
    - `POST /ingest` with `{ url }`.
    - `POST /chat` with `{ question }`. [web:151][web:154]

### 2. Backend (RAG engine)

- **Tech (example setup)**:
  - Python + FastAPI.
  - Any embedding model (OpenAI / local) and vector store (e.g., Qdrant, FAISS, in-memory). [web:147][web:154]
- **Endpoints**:
  - `POST /ingest`
    - Request: `{ "url": "https://example.com/..." }`
    - Steps:
      1. Fetch HTML from the URL.
      2. Extract main article content and code blocks.
      3. Split into chunks.
      4. Compute embeddings and store in a vector index with metadata. [web:147]
    - Response: `{ "msg": "Indexed content from <url>" }`.
  - `POST /chat`
    - Request: `{ "question": "your question here" }`
    - Steps:
      1. Embed the question.
      2. Retrieve top-k similar chunks from the index.
      3. Call an LLM with a prompt that includes the retrieved context and the question.
      4. Instruct the LLM to answer **only** from the provided context. [web:148][attached_file:1]
    - Response: `{ "answer": "LLM-generated answer grounded in the page" }`.

---

## Tech stack

### Frontend

- React
- TypeScript
- Vite or Create React App
- CSS (custom, no UI framework)

###Backend

- Python
- FastAPI (or similar framework)
- Embedding model provider (OpenAI / local embedding model)
- Vector store (FAISS / Qdrant / in-memory for dev) [web:147][web:154]


Roadmap / future improvements

-Support multiple URLs and whole-document collections.
-PDF and file uploads in addition to web pages.
-Show citations/snippets so users can see exactly which paragraph the answer came from.
-Add authentication and user-specific history.
-Deploy the app (frontend + backend) on a cloud platform for demo.
