from pathlib import Path
import os
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

PDF_PATH = Path("data") / "faq.txt"

# AUTO-CREATE SAMPLE FAQ
if not PDF_PATH.exists():
    faq_content = """
RAG FAQ Bot

Q: What is RAG?
A: Retrieval-Augmented Generation. Loads your PDF ‚Üí chunks ‚Üí embeddings ‚Üí FAISS vector search ‚Üí LLM answers from YOUR docs only.

Q: Tech stack?
A: LangChain + Ollama (llama3.1 local) + FAISS + FastAPI + VSCode.

Q: How to add more docs?
A: Put new PDF in data/ ‚Üí rerun ingest.py ‚Üí query /chat endpoint.

Q: Deployment?
A: uvicorn backend.app:app ‚Üí React frontend or curl test.
"""
    os.makedirs("data", exist_ok=True)
    with open(PDF_PATH, "w") as f:
        f.write(faq_content)
    print("‚úÖ Created sample FAQ!")

loader = TextLoader(str(PDF_PATH))
docs = loader.load()
print("üìñ Loaded")

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = splitter.split_documents(docs)
print(f"‚úÇÔ∏è {len(chunks)} chunks")

embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
os.makedirs("vectorstore/faiss_index", exist_ok=True)
vectorstore = FAISS.from_documents(chunks, embeddings)
vectorstore.save_local("vectorstore/faiss_index")
print("‚úÖ VECTOR DB READY! üéâ")
